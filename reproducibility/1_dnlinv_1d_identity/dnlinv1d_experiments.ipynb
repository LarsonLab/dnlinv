{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee9e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import sys\n",
    "sys.path.append('D:\\\\Box\\\\dev\\\\deep_bayesian_recon')\n",
    "import fastMRI.data.transforms as transforms\n",
    "import sigpy as sp\n",
    "import sigpy.plot\n",
    "import sigpy.mri\n",
    "from tqdm.notebook import tqdm\n",
    "from generator_models import calculate_kl\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f2bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, LeakyReLU activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob, norm_layer=nn.BatchNorm2d, norm_momentum=0.1,\n",
    "                 activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            norm_layer(out_chans, momentum=norm_momentum),\n",
    "            nn.ReLU(True) if activation == nn.ReLU else activation(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Conv1d(out_chans, out_chans, kernel_size=3, padding=1, bias=False),\n",
    "            norm_layer(out_chans, momentum=norm_momentum),\n",
    "            nn.ReLU(True) if activation == nn.ReLU else activation(),\n",
    "            nn.Dropout(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transpose Convolutional Block that consists of one convolution transpose layers followed by\n",
    "    instance normalization and LeakyReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob, norm_layer=nn.BatchNorm2d, norm_momentum=0.1,\n",
    "                 activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_chans, out_chans, kernel_size=2, stride=2, bias=False),\n",
    "            norm_layer(out_chans, momentum=norm_momentum),\n",
    "            nn.ReLU(True) if activation == nn.ReLU else activation(),\n",
    "            nn.Dropout(drop_prob),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans})'\n",
    "\n",
    "\n",
    "class UnetModel1d(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model. Adapted from fastMRI GitHub\n",
    "\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234â€“241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_shape, latent_dim, in_chans, out_chans, chans, num_channels, num_pool_layers, drop_prob,\n",
    "                 latent_shape=(0,),\n",
    "                 num_samples=1,\n",
    "                 norm_layer=nn.BatchNorm2d,\n",
    "                 norm_momentum=0.1, max_likelihood=False,\n",
    "                 dip_mc_mode=False,\n",
    "                 dip_mc_dropout=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.latent_shape = img_shape if list(latent_shape) == [0] else latent_shape\n",
    "        \n",
    "        if dip_mc_dropout is False:\n",
    "            z_grad = True if dip_mc_mode is False else False\n",
    "            self.z_mean = nn.Parameter(torch.zeros([self.latent_dim, *self.latent_shape]), requires_grad=z_grad)\n",
    "            self.z_var = nn.Parameter(0.1 * torch.ones([self.latent_dim, *self.latent_shape]), requires_grad=z_grad)\n",
    "        else:\n",
    "        # DIP with Dropout\n",
    "            assert drop_prob > 0, f\"For DIP with MC Dropout, drop_prob needs to be >0. Got {drop_prob}\"\n",
    "            self.z_mean = nn.Parameter(torch.normal(torch.zeros([self.latent_dim, *self.latent_shape]), \n",
    "                                                    1e-2 * torch.ones([self.latent_dim, *self.latent_shape])),\n",
    "                                       requires_grad=False)\n",
    "            self.z_var = nn.Parameter(1e-24 * torch.ones([self.latent_dim, *self.latent_shape]), requires_grad=False)\n",
    "\n",
    "\n",
    "        # Upsample to image shape for U-net to process\n",
    "#         self.latent_upconv = nn.ConvTranspose1d(self.latent_dim, self.in_chans, kernel_size=self.img_shape)\n",
    "\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob, norm_layer=norm_layer,\n",
    "                                                           norm_momentum=norm_momentum)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob, norm_layer=norm_layer,\n",
    "                                                  norm_momentum=norm_momentum)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch * 2, drop_prob, norm_layer=norm_layer,\n",
    "                              norm_momentum=norm_momentum)\n",
    "\n",
    "        coil_latent_dim = ch * 2\n",
    "        ngf = chans\n",
    "\n",
    "        self.up_conv = nn.ModuleList()\n",
    "        self.up_transpose_conv = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch, drop_prob, norm_layer=norm_layer,\n",
    "                                                          norm_momentum=norm_momentum)]\n",
    "            self.up_conv += [ConvBlock(ch * 2, ch, drop_prob, norm_layer=norm_layer,\n",
    "                                       norm_momentum=norm_momentum)]\n",
    "            ch //= 2\n",
    "\n",
    "        self.up_transpose_conv += [TransposeConvBlock(ch * 2, ch, drop_prob, norm_layer=norm_layer,\n",
    "                                                      norm_momentum=norm_momentum)]\n",
    "        self.up_conv += [\n",
    "            nn.Sequential(\n",
    "                ConvBlock(ch * 2, ch, drop_prob=0., norm_layer=norm_layer,\n",
    "                          norm_momentum=norm_momentum),\n",
    "                nn.Conv1d(ch, self.out_chans, kernel_size=1, stride=1),\n",
    "            )]\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.rng = nn.Parameter(torch.zeros([self.num_samples, self.latent_dim, *self.latent_shape]), requires_grad=False)\n",
    "        # self.rng = nn.Parameter(torch.zeros([self.num_samples, self.in_chans, 1, 1]))\n",
    "        self.rng.requires_grad = False\n",
    "\n",
    "        self.max_likelihood = max_likelihood\n",
    "\n",
    "        if self.max_likelihood:\n",
    "            self.z_var.requires_grad = False\n",
    "            self.z_mean.requires_grad = False\n",
    "            self.z_mean.random_()  # Initialize with a sample from a Normal distribution\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.max_likelihood:\n",
    "            z = self.z_mean.unsqueeze(0)\n",
    "        else:\n",
    "            z = self.z_mean + self.z_var ** 0.5 * self.rng.normal_()\n",
    "        # input = F.interpolate(z, size=self.img_shape, mode='bilinear', align_corners=False)\n",
    "#         input = self.latent_upconv(z)\n",
    "        input = z\n",
    "\n",
    "        stack = []\n",
    "        output = input\n",
    "\n",
    "        # Apply down-sampling layers\n",
    "        for i, layer in enumerate(self.down_sample_layers):\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool1d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "\n",
    "            # Reflect pad on the right/botton if needed to handle odd input dimensions.\n",
    "            padding = [0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1 # Padding right\n",
    "            if sum(padding) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "\n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "\n",
    "        return z, output\n",
    "\n",
    "    def infer(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.max_likelihood:\n",
    "            z = self.z_mean.unsqueeze(0)\n",
    "        else:\n",
    "            z = self.z_mean + self.z_var ** 0.5 * self.rng[0:1].normal_()\n",
    "        # input = F.interpolate(z, size=self.img_shape, mode='bilinear', align_corners=False)\n",
    "#         input = self.latent_upconv(z)\n",
    "        input = z\n",
    "\n",
    "        stack = []\n",
    "        output = input\n",
    "\n",
    "        # Apply down-sampling layers\n",
    "        for i, layer in enumerate(self.down_sample_layers):\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.avg_pool1d(output, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n",
    "            downsample_layer = stack.pop()\n",
    "            output = transpose_conv(output)\n",
    "\n",
    "            # Reflect pad on the right/botton if needed to handle odd input dimensions.\n",
    "            padding = [0, 0]\n",
    "            if output.shape[-1] != downsample_layer.shape[-1]:\n",
    "                padding[1] = 1  # Padding right\n",
    "            if sum(padding) != 0:\n",
    "                output = F.pad(output, padding, \"reflect\")\n",
    "\n",
    "            output = torch.cat([output, downsample_layer], dim=1)\n",
    "            output = conv(output)\n",
    "        \n",
    "        return z, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c41463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModelCoilEstimated(nn.Module):\n",
    "    def __init__(self, noise_sigma, num_channels, img_shape, mask, maximum_likelihood=False, \n",
    "                 no_noise_f_reg=False,\n",
    "                 device='cpu',\n",
    "                 n_mps=1):\n",
    "        super(ForwardModelCoilEstimated, self).__init__()\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "        self.img_shape = img_shape\n",
    "        self.device = device\n",
    "        self.mask = mask.to(device)\n",
    "        self.n_mps = n_mps\n",
    "        self.rng = torch.zeros([self.n_mps,\n",
    "                                self.num_channels,\n",
    "                                self.img_shape[0],\n",
    "                                2]).to(device)\n",
    "        self.rng.requires_grad = False\n",
    "        self.maximum_likelihood = maximum_likelihood\n",
    "        self.no_noise_f_reg = no_noise_f_reg\n",
    "\n",
    "\n",
    "    def forward(self, image, coil_est, noise_sigma):\n",
    "        if self.maximum_likelihood or self.no_noise_f_reg:\n",
    "            y = transforms.fft1(transforms.complex_mul(coil_est, image.unsqueeze(2)))\n",
    "            y = torch.sum(y, dim=1)  # Reduce Soft SENSE dim\n",
    "        else:\n",
    "            y = transforms.fft1(transforms.complex_mul(coil_est, image.unsqueeze(2)))\n",
    "            y = torch.sum(y, dim=1)  # Reduce Soft SENSE dim\n",
    "            y = y + noise_sigma * self.rng.normal_()\n",
    "\n",
    "        return y[self.mask.expand_as(y)].reshape(image.shape[0], self.num_channels, -1)\n",
    "    \n",
    "\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self, img_shape, num_channels, device, maximum_likelihood=False, no_noise_f_reg=False):\n",
    "        super(ForwardModel, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        self.num_channels = num_channels\n",
    "        self.rng = torch.zeros([1,\n",
    "                                self.num_channels,\n",
    "                                self.img_shape[0],\n",
    "                                1]).to(device)\n",
    "        self.rng.requires_grad = False\n",
    "        self.maximum_likelihood = maximum_likelihood\n",
    "        self.no_noise_f_reg = no_noise_f_reg\n",
    "        \n",
    "        \n",
    "    def forward(self, data, noise_sigma):\n",
    "        if self.maximum_likelihood or self.no_noise_f_reg:\n",
    "            y = data + 0.0 * self.rng.normal_()\n",
    "        else:\n",
    "            y = data + noise_sigma * self.rng.normal_()\n",
    "        return y.reshape(data.shape[0], self.num_channels, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5891a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationModel(nn.Module):\n",
    "    def __init__(self, y_data, forward_model, generative_model, img_shape, latent_dim, noise_sigma, device,\n",
    "                 num_samples=10, max_likelihood=False, coil_init=False, existing_mps=None, l2_reg=0,\n",
    "                 n_mps=1,\n",
    "                 noise_estimation=True):\n",
    "        # Latent variables: A, P\n",
    "        # Parameters: lamda (prior precision on A), P_prior_mean, P_prior_var\n",
    "        # Variational parameters: P_params\n",
    "        # Observed variables: y\n",
    "        super(OptimizationModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.img_shape = img_shape + (1,)\n",
    "        self.log_noise_sigma = nn.Parameter(torch.log(torch.tensor(noise_sigma)), requires_grad=noise_estimation)  # Noise standard deviation\n",
    "        self.y_data = y_data.to(self.device)  # Store current observed data\n",
    "\n",
    "        # Priors for latent variable z\n",
    "        self.z_prior_mean = torch.zeros([latent_dim])\n",
    "        self.z_prior_var = torch.ones([latent_dim])\n",
    "\n",
    "        # Specify generative network\n",
    "        # self.g = NNGenerator(latent_dim=latent_dim, img_shape=img_shape)\n",
    "        self.g = generative_model\n",
    "\n",
    "        # Specify computational parameters\n",
    "        self.latent_dim = latent_dim  # Number of latent variables\n",
    "        self.num_samples = num_samples  # Number of samples to approximate expectations\n",
    "        self.N = np.prod(self.img_shape)  # Number of voxels in image\n",
    "\n",
    "        # Specify forward model\n",
    "        self.f = forward_model.to(device)\n",
    "        self.f.eval()\n",
    "        self.f.device = device\n",
    "        for param in self.f.parameters():\n",
    "            param.to(device)\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.max_likelihood = max_likelihood\n",
    "        self.coil_init = coil_init\n",
    "\n",
    "        self.existing_mps = existing_mps\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "        self.n_mps = n_mps\n",
    "\n",
    "        self.sigma_eps = 1e-9\n",
    "\n",
    "        self.scale_factor = nn.Parameter(torch.tensor(0.0))\n",
    "        # self.scale_factor = nn.Parameter(torch.zeros([1, self.n_mps, self.g.num_channels // self.n_mps, 1, 1, 1]))\n",
    "\n",
    "    def forward(self):\n",
    "        # Get image estimate\n",
    "        z, x = self.g()\n",
    "        x = x.reshape(self.num_samples, 1, self.n_mps, self.img_shape[0])\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "        # Get noise sigma from log-noise-sigma\n",
    "        self.noise_sigma = torch.exp(self.log_noise_sigma)\n",
    "\n",
    "        x = x * torch.exp(self.scale_factor)\n",
    "        y = self.f(x, self.noise_sigma)\n",
    "\n",
    "        if self.max_likelihood:\n",
    "            # Calculate loss\n",
    "            mse = torch.sum((self.y_data.expand_as(y) - y)**2)\n",
    "            loss = mse + self.l2_reg * (torch.sum(x ** 2))\n",
    "            log_likelihood = torch.tensor([0.], requires_grad=False)\n",
    "            kl = torch.tensor([0.], requires_grad=False)\n",
    "        else:\n",
    "            # Calculate loss\n",
    "\n",
    "            mse = torch.sum(\n",
    "                torch.mean((self.y_data.expand_as(y) - y) ** 2, dim=0))\n",
    "\n",
    "            log_likelihood = -0.5 * torch.numel(y) * (\n",
    "                torch.log(torch.tensor(2 * np.pi).to(self.device)) + 2 * (self.log_noise_sigma + self.sigma_eps)) \\\n",
    "                             - 0.5 * (torch.exp(-2 * (self.log_noise_sigma + self.sigma_eps))) * mse\n",
    "\n",
    "            kl = calculate_kl(self.g.z_mean, torch.sqrt(torch.abs(self.g.z_var)), 0.0, 1.0)\n",
    "\n",
    "            reg = self.l2_reg * (torch.sum(x**2))\n",
    "\n",
    "            if hasattr(self.g, 'calculate_elbo_entropy'):\n",
    "                kl += self.g.calculate_elbo_entropy()\n",
    "                # qz_entropy += qw\n",
    "\n",
    "            ELBO = log_likelihood - kl # log_z_prior - qz_entropy\n",
    "            loss = -ELBO + reg\n",
    "\n",
    "        return loss, log_likelihood, kl, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1740ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "img_shape = (256,)\n",
    "num_channels = 8\n",
    "num_samples = 1\n",
    "n_mps = 1\n",
    "\n",
    "t = np.linspace(-1, 1, img_shape[0])\n",
    "x_in = np.cos(2 * np.pi * 1.5 * t).astype(np.float32) * 0.5\n",
    "x_in[(t > 0.5) & (t <= 1)] = 2*t[(t > 0.5) & (t <= 1)] - 1\n",
    "x_in[(t < -0.5) & (t >= -1)] = 2*t[(t < -0.5) & (t >= -1)] + 1\n",
    "x_in -= x_in.min()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_in)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'x_input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9ed3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "x_tensor = transforms.to_tensor(x_in)\n",
    "x_tensor = x_tensor.resize(1, 1, img_shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa14b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 34433\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "noise_sigma = 0.1\n",
    "mask = torch.ones([img_shape[0], 2], dtype=torch.bool)\n",
    "device = 'cpu'\n",
    "\n",
    "f = ForwardModel(img_shape=img_shape, num_channels=num_channels, device=device)\n",
    "y_tensor = f.forward(x_tensor, noise_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc5101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_tensor.squeeze().detach().cpu().numpy().T, '.')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'y_data_noise_{noise_sigma}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8986b881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bb24ee3c50>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLE estimator\n",
    "x_inverse = torch.mean(y_tensor, dim=1)\n",
    "plt.figure()\n",
    "plt.plot(x_inverse.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc23d13",
   "metadata": {},
   "source": [
    "# Per iteration error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a54a2506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\dev\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30d8900c73244588b1cc90c32b4c5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dip_mc_inference\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca58693b80b47c58b5ee20233cc57fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dip_mc_dropout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6404144b36de41b99c7432061e1edc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dip_mc_z_mc_dropout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ae56e3e8c64e88b426a07cabe04bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dnlinv_no_dropout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079da58308c54158ab44b9a961c4f7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running estimation with dnlinv_dropout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1623ef8d14174d91a52be368d98f870a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimize q(z):   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chans, num_pool_layers = [32, 4]\n",
    "chans = int(chans)\n",
    "num_pool_layers = int(num_pool_layers)\n",
    "latent_dim = 16\n",
    "in_chans = latent_dim\n",
    "out_chans = 1\n",
    "drop_prob = 0.\n",
    "\n",
    "infer_num_samples = 128\n",
    "\n",
    "max_iter = 10000\n",
    "calc_error_iter = 50\n",
    "\n",
    "device = 'cuda'\n",
    "l2_reg = 0\n",
    "\n",
    "\n",
    "weight_decay = 1e-2\n",
    "lr = 1e-4\n",
    "\n",
    "seed = 34433\n",
    "\n",
    "mean_scale = torch.mean(y_tensor)\n",
    "y_tensor_scaled = y_tensor - mean_scale\n",
    "std_scale = torch.std(y_tensor_scaled)\n",
    "y_tensor_scaled = y_tensor_scaled / std_scale\n",
    "\n",
    "noise_sigma_init = noise_sigma / std_scale\n",
    "\n",
    "ref_data = x_tensor\n",
    "\n",
    "modes = ['dip', 'dip_mc_inference', 'dip_mc_dropout', 'dip_mc_z_mc_dropout', 'dnlinv_no_dropout', 'dnlinv_dropout']\n",
    "#modes = ['dip', 'dnlinv_no_dropout', 'dnlinv_dropout']\n",
    "# modes = ['dip', 'dip_mc_z_mc_dropout']\n",
    "mode_error = {}\n",
    "x_est = {}\n",
    "mode_losses = {}\n",
    "x_mc_est = {}\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"Running estimation with {mode}\")\n",
    "    if mode == 'dip':\n",
    "        maximum_likelihood = True\n",
    "        dip_mc_mode=False\n",
    "        dip_mc_dropout=False\n",
    "        noise_estimation=False\n",
    "        no_noise_f_reg = True\n",
    "        drop_prob = 0.\n",
    "    elif mode == 'dip_mc_inference':\n",
    "        maximum_likelihood = False\n",
    "        dip_mc_mode=True\n",
    "        dip_mc_dropout=False\n",
    "        noise_estimation=False\n",
    "        no_noise_f_reg = True\n",
    "        drop_prob = 0.\n",
    "    elif mode == 'dip_mc_dropout':\n",
    "        maximum_likelihood = False\n",
    "        dip_mc_mode=False\n",
    "        dip_mc_dropout=True\n",
    "        noise_estimation=False\n",
    "        no_noise_f_reg = True\n",
    "        drop_prob = 0.1\n",
    "    elif mode == 'dip_mc_z_mc_dropout':\n",
    "        maximum_likelihood = False\n",
    "        dip_mc_mode=True\n",
    "        dip_mc_dropout=True\n",
    "        noise_estimation=False\n",
    "        no_noise_f_reg = True\n",
    "        drop_prob = 0.1\n",
    "    elif mode == 'dnlinv_no_dropout':\n",
    "        maximum_likelihood = False\n",
    "        dip_mc_mode=False\n",
    "        dip_mc_dropout=False\n",
    "        noise_estimation=True\n",
    "        no_noise_f_reg = False\n",
    "        drop_prob = 0.0\n",
    "    elif mode == 'dnlinv_dropout':\n",
    "        maximum_likelihood = False\n",
    "        dip_mc_mode=False\n",
    "        dip_mc_dropout=False\n",
    "        noise_estimation=True\n",
    "        no_noise_f_reg = False\n",
    "        drop_prob = 0.1\n",
    "\n",
    "    num_samples = 16 if maximum_likelihood is False else 1\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    g = UnetModel1d(img_shape, latent_dim, in_chans, out_chans, chans, num_channels, num_pool_layers, drop_prob,\n",
    "             num_samples=num_samples,\n",
    "             norm_layer=nn.InstanceNorm1d,\n",
    "             norm_momentum=0.1, max_likelihood=maximum_likelihood,\n",
    "             dip_mc_mode=dip_mc_mode,\n",
    "             dip_mc_dropout=dip_mc_dropout)\n",
    "\n",
    "\n",
    "\n",
    "    f = ForwardModel(img_shape=img_shape, num_channels=num_channels, device=device, maximum_likelihood=maximum_likelihood,\n",
    "                    no_noise_f_reg=no_noise_f_reg)\n",
    "\n",
    "    my_model = OptimizationModel(y_data=y_tensor_scaled, forward_model=f, generative_model=g, img_shape=img_shape,\n",
    "                                         latent_dim=latent_dim,\n",
    "                                         noise_sigma=noise_sigma_init, device=device, num_samples=num_samples,\n",
    "                                         max_likelihood=maximum_likelihood, existing_mps=None,\n",
    "                                         l2_reg=l2_reg, n_mps=n_mps,\n",
    "                                noise_estimation=noise_estimation)\n",
    "    my_model = my_model.to(device)\n",
    "\n",
    "\n",
    "    optim = torch.optim.AdamW([{'params': my_model.g.parameters(), 'weight_decay': weight_decay},\n",
    "                                              #{'params': my_model.log_noise_sigma},\n",
    "                                              {'params': my_model.scale_factor}],\n",
    "                                             lr=lr)\n",
    "\n",
    "    # Run optimization\n",
    "\n",
    "    err = []\n",
    "    losses = np.zeros(max_iter)\n",
    "    with tqdm(total=max_iter, desc='Optimize q(z)') as pbar:\n",
    "        for i in range(max_iter):\n",
    "            loss, log_likelihood, kl, mse = my_model.forward()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            optim.step()\n",
    "\n",
    "            pbar.set_postfix(iteration=i, loss=loss.item(), neg_log_likelihood=-log_likelihood.item(),\n",
    "                             kl=kl.item(), sse=mse.item())\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "            # Do inference for error calculation\n",
    "            if (i % calc_error_iter) == 0:\n",
    "                # Do inference\n",
    "                with torch.no_grad():\n",
    "                    if maximum_likelihood:\n",
    "                        z, x = my_model.g()\n",
    "                        x = x.reshape(num_samples, 1, n_mps, img_shape[0])\n",
    "                        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "                    else:\n",
    "                        # Generate estimate from Monte-Carlo\n",
    "                        n_samples = infer_num_samples\n",
    "                        x = []\n",
    "                        for i in range(n_samples):\n",
    "                            z, x_mc = my_model.g.infer()\n",
    "                            x.append(x_mc.to('cpu'))\n",
    "\n",
    "                        x = torch.cat(x, dim=0).reshape(n_samples, 1, n_mps, img_shape[0]).permute(0, 2, 3, 1).contiguous()\n",
    "                    x *= torch.exp(my_model.scale_factor.cpu())\n",
    "                    x *= std_scale\n",
    "                    x += mean_scale\n",
    "                    x_mean = torch.mean(x, dim=0)\n",
    "\n",
    "                    error = x_mean - ref_data.resize(img_shape[0], 1).type_as(x)\n",
    "                    err.append(error.detach().cpu().numpy().flatten())\n",
    "\n",
    "            losses[i] = loss.item()\n",
    "            # Get error per few iters\n",
    "    err = np.stack(err, axis=0)\n",
    "\n",
    "    mode_error[mode] = err\n",
    "    x_est[mode] = x_mean\n",
    "    mode_losses[mode] = losses\n",
    "    x_mc_est[mode] = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecac3406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR MLE: 30.924830436706543\n"
     ]
    }
   ],
   "source": [
    "reference = (x_tensor).resize(img_shape[0], 1).numpy().flatten()\n",
    "x_gaussian_mean_est = (x_inverse.T).detach().squeeze().numpy().flatten()\n",
    "mse = np.sqrt(np.mean((x_gaussian_mean_est - reference)**2))\n",
    "psnr_mean_gaussian = 10 * np.log10(np.max(reference ** 2) / mse)\n",
    "print(f'PSNR MLE: {psnr_mean_gaussian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c9ce211",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = (x_tensor).resize(img_shape[0], 1).numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "for mode in modes:\n",
    "    mse = np.sqrt(np.mean(mode_error[mode]**2, axis=(1)))\n",
    "    psnr = 10 * np.log10(np.max(reference ** 2) / mse)\n",
    "    nrmse = np.sqrt(mse) / np.sqrt(np.mean(reference**2))\n",
    "    plt.plot(range(0, max_iter, calc_error_iter)[:len(psnr)], psnr)\n",
    "\n",
    "plt.title('PSNR vs iterations')\n",
    "plt.ylabel('PSNR (dB)')\n",
    "plt.xlabel('iterations')\n",
    "plt.axhline(psnr_mean_gaussian, linestyle='--', color='m', linewidth=1.0)\n",
    "plt.legend(modes + ['MLE'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'PSNR_iterations_{noise_sigma}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92d5a34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\dev\\lib\\site-packages\\torch\\tensor.py:447: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "ref = (x_tensor).resize(img_shape[0], 1).squeeze()\n",
    "plt.figure(figsize=(16, 9))\n",
    "for idx, mode in enumerate(modes):\n",
    "    est = (x_est[mode]).detach().cpu().numpy().squeeze()\n",
    "    plt.subplot(2,len(modes),idx+1)\n",
    "    plt.plot(est, linewidth=2.0)\n",
    "    plt.title(mode)\n",
    "    plt.plot(ref, '--', linewidth=1.0)\n",
    "    plt.subplot(2,len(modes),idx+1+len(modes))\n",
    "    plt.plot(ref-est)\n",
    "    plt.title(f\"{mode} error\")\n",
    "# plt.legend(modes)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'reconstructions_{noise_sigma}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453b697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}